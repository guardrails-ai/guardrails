{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4d10ab",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is a comprehensive guide on integrating Guardrails with [LangChain](https://github.com/langchain-ai/langchain), a framework for developing applications powered by large language models. By combining the validation capabilities of Guardrails with the flexible architecture of LangChain, you can create reliable and robust AI applications.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Easy Integration**: Guardrails can be seamlessly added to LangChain's LCEL syntax, allowing for quick implementation of validation checks.\n",
    "- **Flexible Validation**: Guardrails provides various validators that can be used to enforce structural, type, and quality constraints on LLM outputs.\n",
    "- **Corrective Actions**: When validation fails, Guardrails can take corrective measures, such as retrying LLM prompts or fixing outputs.\n",
    "- **Compatibility**: Works with different LLMs and can be used in various LangChain components like chains, agents, and retrieval strategies.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Ensure you have the following langchain packages installed. Also install Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382bd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "!  pip install guardrails-ai langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594ef6c",
   "metadata": {},
   "source": [
    "2. As a prerequisite we install the necessary validators from the Guardrails Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05635d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! guardrails hub install hub://guardrails/competitor_check --quiet\n",
    "! guardrails hub install hub://guardrails/toxic_language --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6fdd9",
   "metadata": {},
   "source": [
    "- `CompetitorCheck`: Identifies and optionally removes mentions of specified competitor names.\n",
    "- `ToxicLanguage`: Detects and optionally removes toxic or inappropriate language from the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a69d3",
   "metadata": {},
   "source": [
    "## Basic Integration\n",
    "\n",
    "Here's a basic example of how to integrate Guardrails with a LangChain LCEL chain:\n",
    "\n",
    "1. Import the required imports and do the OpenAI Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae149cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701d9e4",
   "metadata": {},
   "source": [
    "2. Create a Guard object with two validators: CompetitorCheck and ToxicLanguage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f810db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CompetitorCheck' from 'guardrails.hub' (/Users/calebcourier/Projects/guardrails/.venv/lib/python3.10/site-packages/guardrails/hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mguardrails\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Guard\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mguardrails\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompetitorCheck, ToxicLanguage\n\u001b[1;32m      4\u001b[0m competitors_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamerican airlines\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munited\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m guard \u001b[38;5;241m=\u001b[39m Guard()\u001b[38;5;241m.\u001b[39muse_many(\n\u001b[1;32m      6\u001b[0m     CompetitorCheck(competitors\u001b[38;5;241m=\u001b[39mcompetitors_list, on_fail\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfix\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     ToxicLanguage(on_fail\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CompetitorCheck' from 'guardrails.hub' (/Users/calebcourier/Projects/guardrails/.venv/lib/python3.10/site-packages/guardrails/hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "from guardrails import Guard\n",
    "from guardrails.hub import CompetitorCheck, ToxicLanguage\n",
    "\n",
    "competitors_list = [\"delta\", \"american airlines\", \"united\"]\n",
    "guard = Guard().use_many(\n",
    "    CompetitorCheck(competitors=competitors_list, on_fail=\"fix\"),\n",
    "    ToxicLanguage(on_fail=\"filter\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff173e9",
   "metadata": {},
   "source": [
    "3. Define the LCEL chain components and pipe the prompt, model, output parser, and the Guard together.\n",
    "The `guard.to_runnable()` method converts the Guardrails guard into a LangChain-compatible runnable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Answer this question {question}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | guard.to_runnable() | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293a6f7",
   "metadata": {},
   "source": [
    "4. Invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c037923",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"question\": \"What are the top five airlines for domestic travel in the US?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4c67b",
   "metadata": {},
   "source": [
    "Example output:\n",
    "    ```\n",
    "    1. Southwest Airlines\n",
    "    3. JetBlue Airways\n",
    "    ```\n",
    "\n",
    "In this example, the chain sends the question to the model and then applies Guardrails validators to the response. The CompetitorCheck validator specifically removes mentions of the specified competitors (Delta, American Airlines, United), resulting in a filtered list of non-competitor airlines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
