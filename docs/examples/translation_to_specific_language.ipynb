{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate text without profanities\n",
    "\n",
    "!!! note\n",
    "    To download this example as a Jupyter notebook, click [here](https://github.com/ShreyaR/guardrails/blob/main/docs/examples/translation_to_specific_language.ipynb).\n",
    "\n",
    "In this example, we will use Guardrails during the translation of a statement from another language to english. We will check whether the translated statement passes the profanity check or not.\n",
    "\n",
    "## Objective\n",
    "\n",
    "We want to translate a statement from another languages to English and ensure the translated statement is profanity free.\n",
    "\n",
    "## Step 0: Setup\n",
    "\n",
    "In order to run this example, you will need to install `alt-profanity-check` package. You can do so by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install alt-profanity-check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the RAIL Spec\n",
    "\n",
    "Ordinarily, we would create an RAIL spec in a separate file. For the purposes of this example, we will create the spec in this notebook as a string following the RAIL syntax. For more information on RAIL, see the [RAIL documentation](../rail/output.md).\n",
    "\n",
    "In this RAIL spec, we:\n",
    "\n",
    "1. Create an `output` schema that returns a single key-value pair. The key should be 'translated_statement', and the value should be the English translation of the given statement. The translated statement should not have any profanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from profanity_check import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rail_str = \"\"\"\n",
    "<rail version=\"0.1\">\n",
    "\n",
    "<script language='python'>\n",
    "from profanity_check import predict\n",
    "from guardrails.validators import Validator, EventDetail, register_validator\n",
    "\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "@register_validator(name=\"is-profanity-free\", data_type=\"string\")\n",
    "class IsProfanityFree(Validator):\n",
    "    global predict\n",
    "    global EventDetail\n",
    "    def validate(self, key, value, schema) -> Dict:\n",
    "        text = value\n",
    "        prediction = predict([value])\n",
    "        if prediction[0] == 1:\n",
    "            raise EventDetail(\n",
    "                key,\n",
    "                value,\n",
    "                schema,\n",
    "                f\"Value {value} contains profanity language\",\n",
    "                \"\",\n",
    "            )\n",
    "        return schema\n",
    "</script>\n",
    "\n",
    "<output>\n",
    "    <string\n",
    "        name=\"translated_statement\"\n",
    "        description=\"Translate the given statement into english language\"\n",
    "        format=\"is-profanity-free\"\n",
    "        on-fail-is-profanity-free=\"fix\" \n",
    "    />\n",
    "</output>\n",
    "\n",
    "\n",
    "<prompt>\n",
    "\n",
    "Translate the given statement into english language:\n",
    "\n",
    "${statement_to_be_translated}\n",
    "\n",
    "${complete_json_suffix}\n",
    "</prompt>\n",
    "\n",
    "\n",
    "</rail>\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "\n",
    "    In order to ensure the translated statement is profanity free, we use `is-profanity-free` as the validator. This validator uses `profanity_check` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a `Guard` object with the RAIL Spec\n",
    "\n",
    "We create a `gd.Guard` object that will check, validate and correct the output of the LLM. This object:\n",
    "\n",
    "1. Enforces the quality criteria specified in the RAIL spec.\n",
    "2. Takes corrective action when the quality criteria are not met.\n",
    "3. Compiles the schema and type info from the RAIL spec and adds it to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import guardrails as gd\n",
    "\n",
    "from rich import print\n",
    "\n",
    "guard = gd.Guard.from_rail_string(rail_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the prompt that will be sent to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(guard.base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `statement_to_be_translated` is the the statement and will be provided by the user at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Wrap the LLM API call with `Guard`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try translating a statement that doesn't have any profanity in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "raw_llm_response, validated_response = guard(\n",
    "    openai.Completion.create,\n",
    "    prompt_params={\"statement_to_be_translated\": \"quesadilla de pollo\"},\n",
    "    engine=\"text-davinci-003\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the output of the LLM and the validated output using the Guard's internal logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.state.most_recent_call.tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `guard` wrapper returns the raw_llm_respose (which is a simple string), and the validated and corrected output (which is a dictionary). We can see that the output is a dictionary with the correct schema and types.\n",
    "\n",
    "Next, let's try translating a statement that has profanity in it. We see that the translated statement has been corrected to return an empty string instead of the translated statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_llm_response, validated_response = guard(\n",
    "    openai.Completion.create,\n",
    "    prompt_params={\"statement_to_be_translated\": \"убей себя\"},\n",
    "    engine=\"text-davinci-003\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f\"Validated Output: {validated_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, when we look at the logs, we can see that the output of the LLM was filtered out because it did not pass the profanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guard.state.most_recent_call.tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiff-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
